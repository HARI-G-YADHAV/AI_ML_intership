{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbafede3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Dr. John’s AI startup, founded in 2018, has revolutionized the industry. \n",
    "\"Isn’t it amazing?\" said Sarah, who'd invested $1.2 million. \n",
    "Meanwhile, others weren’t convinced—it’s risky, after all! \n",
    "Running, ran, and runs are just different forms of the word 'run'.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8214139c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/itachi_uchiha/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "af250804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_token : \n",
      " ['Dr.', 'John', '’', 's', 'AI', 'startup', ',', 'founded', 'in', '2018', ',', 'has', 'revolutionized', 'the', 'industry', '.', '``', 'Isn', '’', 't', 'it', 'amazing', '?', \"''\", 'said', 'Sarah', ',', 'who', \"'d\", 'invested', '$', '1.2', 'million', '.', 'Meanwhile', ',', 'others', 'weren', '’', 't', 'convinced—it', '’', 's', 'risky', ',', 'after', 'all', '!', 'Running', ',', 'ran', ',', 'and', 'runs', 'are', 'just', 'different', 'forms', 'of', 'the', 'word', \"'run\", \"'\", '.']\n",
      "sent_token : \n",
      " ['\\nDr. John’s AI startup, founded in 2018, has revolutionized the industry.', '\"Isn’t it amazing?\"', \"said Sarah, who'd invested $1.2 million.\", 'Meanwhile, others weren’t convinced—it’s risky, after all!', \"Running, ran, and runs are just different forms of the word 'run'.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "\n",
    "token_word = word_tokenize(text)\n",
    "token_sent = sent_tokenize(text)\n",
    "\n",
    "print(\"word_token : \\n\",token_word)\n",
    "print(\"sent_token : \\n\",token_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c033fc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords removed : \n",
      " ['John', 'AI', 'startup', 'founded', 'revolutionized', 'industry', 'amazing', 'said', 'Sarah', 'invested', 'million', 'Meanwhile', 'others', 'risky', 'Running', 'ran', 'runs', 'different', 'forms', 'word']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "filtered_nltk = []\n",
    "\n",
    "for word in token_word:\n",
    "    word_lower = word.lower()\n",
    "    if word_lower not in stop_words_nltk and word.isalpha():\n",
    "        filtered_nltk.append(word)\n",
    "\n",
    "print(\"stopwords removed : \\n\",filtered_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09a9de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized nltk:  ['John', 'AI', 'startup', 'founded', 'revolutionized', 'industry', 'amazing', 'said', 'Sarah', 'invested', 'million', 'Meanwhile', 'others', 'risky', 'Running', 'ran', 'run', 'different', 'form', 'word']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_nltk = []\n",
    "\n",
    "for word in filtered_nltk:\n",
    "    lemma = lemmatizer.lemmatize(word)\n",
    "    lemmatized_nltk.append(lemma)\n",
    "    \n",
    "print(\"lemmatized nltk: \",lemmatized_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00149512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['\\n', 'Dr.', 'John', '’s', 'AI', 'startup', ',', 'founded', 'in', '2018', ',', 'has', 'revolutionized', 'the', 'industry', '.', '\\n', '\"', 'Is', 'n’t', 'it', 'amazing', '?', '\"', 'said', 'Sarah', ',', 'who', \"'d\", 'invested', '$', '1.2', 'million', '.', '\\n', 'Meanwhile', ',', 'others', 'were', 'n’t', 'convinced', '—', 'it', '’s', 'risky', ',', 'after', 'all', '!', '\\n', 'Running', ',', 'ran', ',', 'and', 'runs', 'are', 'just', 'different', 'forms', 'of', 'the', 'word', \"'\", 'run', \"'\", '.', '\\n']\n",
      "\n",
      "Filtered Tokens (no stopwords, no punctuation):\n",
      "['\\n', 'Dr.', 'John', 'AI', 'startup', 'founded', '2018', 'revolutionized', 'industry', '\\n', 'amazing', 'said', 'Sarah', 'invested', '$', '1.2', 'million', '\\n', 'convinced', 'risky', '\\n', 'Running', 'ran', 'runs', 'different', 'forms', 'word', 'run', '\\n']\n",
      "\n",
      "Lemmatized Tokens:\n",
      "['\\n', 'Dr.', 'John', 'AI', 'startup', 'found', '2018', 'revolutionize', 'industry', '\\n', 'amazing', 'say', 'Sarah', 'invest', '$', '1.2', 'million', '\\n', 'convince', 'risky', '\\n', 'running', 'run', 'run', 'different', 'form', 'word', 'run', '\\n']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Dr. John’s AI startup, founded in 2018, has revolutionized the industry. \n",
    "\"Isn’t it amazing?\" said Sarah, who'd invested $1.2 million. \n",
    "Meanwhile, others weren’t convinced—it’s risky, after all! \n",
    "Running, ran, and runs are just different forms of the word 'run'.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "doc = nlp(text)\n",
    "tokens = []\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "\n",
    "\n",
    "filtered_tokens = []\n",
    "for token in doc:\n",
    "    if not token.is_stop and not token.is_punct:\n",
    "        filtered_tokens.append(token)\n",
    "\n",
    "\n",
    "lemmatized_tokens = []\n",
    "for token in filtered_tokens:\n",
    "    lemmatized_tokens.append(token.lemma_)\n",
    "\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "print(\"\\nFiltered Tokens (no stopwords, no punctuation):\")\n",
    "print([token.text for token in filtered_tokens])\n",
    "\n",
    "print(\"\\nLemmatized Tokens:\")\n",
    "print(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6584083f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/itachi_uchiha/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/itachi_uchiha/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/itachi_uchiha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/itachi_uchiha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "text = \"\"\"\n",
    "Dr. John’s AI startup, founded in 2018, has revolutionized the industry. \n",
    "\"Isn’t it amazing?\" said Sarah, who'd invested $1.2 million. \n",
    "Meanwhile, others weren’t convinced—it’s risky, after all! \n",
    "Running, ran, and runs are just different forms of the word 'run'.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a9101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dr.', 'NNP'), ('John', 'NNP'), ('’', 'NNP'), ('s', 'NN'), ('AI', 'NNP'), ('startup', 'NN'), (',', ','), ('founded', 'VBN'), ('in', 'IN'), ('2018', 'CD'), (',', ','), ('has', 'VBZ'), ('revolutionized', 'VBN'), ('the', 'DT'), ('industry', 'NN'), ('.', '.'), ('``', '``'), ('Isn', 'NNP'), ('’', 'NNP'), ('t', 'VBD'), ('it', 'PRP'), ('amazing', 'VBG'), ('?', '.'), (\"''\", \"''\"), ('said', 'VBD'), ('Sarah', 'NNP'), (',', ','), ('who', 'WP'), (\"'d\", 'VBD'), ('invested', 'VBN'), ('$', '$'), ('1.2', 'CD'), ('million', 'CD'), ('.', '.'), ('Meanwhile', 'RB'), (',', ','), ('others', 'NNS'), ('weren', 'VBP'), ('’', 'JJ'), ('t', 'NN'), ('convinced—it', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('risky', 'NN'), (',', ','), ('after', 'IN'), ('all', 'DT'), ('!', '.'), ('Running', 'NNP'), (',', ','), ('ran', 'NN'), (',', ','), ('and', 'CC'), ('runs', 'NNS'), ('are', 'VBP'), ('just', 'RB'), ('different', 'JJ'), ('forms', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('word', 'NN'), (\"'run\", 'POS'), (\"'\", 'POS'), ('.', '.')] \n",
      "\n",
      "Lemmatized Output:\n",
      "['john', 'ai', 'startup', 'found', 'revolutionize', 'industry', 'amaze', 'say', 'sarah', 'invest', 'million', 'meanwhile', 'others', 'risky', 'running', 'ran', 'run', 'different', 'form', 'word']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize & POS tag\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "print(pos_tags,\"\\n\")\n",
    "\n",
    "# Lemmatize with POS, remove stopwords and punctuation\n",
    "lemmatized = []\n",
    "for word, tag in pos_tags:\n",
    "    if word.isalpha() and word.lower() not in stop_words:\n",
    "        pos = get_wordnet_pos(tag)\n",
    "        lemma = lemmatizer.lemmatize(word, pos)\n",
    "        lemmatized.append(lemma.lower()) \n",
    "\n",
    "\n",
    "print(\"Lemmatized Output:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d04fb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
